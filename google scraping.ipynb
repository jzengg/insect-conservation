{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import requests\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from serpapi.google_scholar_search_results import GoogleScholarSearchResults\n",
    "from serpapi.google_search_results import GoogleSearchResults\n",
    "\n",
    "SERP_API_KEY = os.environ['SERP_API_KEY']\n",
    "GoogleScholarSearchResults.SERP_API_KEY = SERP_API_KEY\n",
    "GoogleSearchResults.SERP_API_KEY = SERP_API_KEY\n",
    "# intermediate csvs\n",
    "SPECIES_FILE = \"data/common_names.csv\"\n",
    "MAIN_NAMES_FILE = \"data/main_common_names.csv\"\n",
    "INSECTS_FILE = \"data/insects.csv\"\n",
    "RAW_FILE = \"redlist_data/simple_summary.csv\"\n",
    "INPUT_FILE = \"data/cleaned_common_names.csv\"\n",
    "INSECTS_WITH_COMMON_NAME_FILE = \"data/insects_with_common_name.csv\"\n",
    "OUTPUT_FILE = \"data/insect_search_and_scholar_counts.csv\"\n",
    "SEARCH_RESULT_DUMP = \"data/insect_search_result_dump.csv\"\n",
    "TWITTER_RESULTS = \"data/twitter_results.csv\"\n",
    "TWITTER_RESULTS_WITH_GOOGLE = \"data/family_with_twitter_and_google_results.csv\"\n",
    "\n",
    "# result csvs\n",
    "IUCN_INSECTS_WITH_BOOKS = \"data/insect_search_and_scholar_counts_and_books.csv\"\n",
    "TWITTER_RESULTS_WITH_BOOKS = \"data/family_with_twitter_google_search_and_books.csv\"\n",
    "FAMILY_GOOGLE_DUMP = \"data/family_google_dump.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make API calls to SERP API for Google Search and Google Scholar\n",
    "\n",
    "def get_num_scholar_results(name):\n",
    "    client = GoogleScholarSearchResults({\"q\": '\"{}\"'.format(name), \"num\": 20})\n",
    "    data = client.get_json()\n",
    "    try:\n",
    "        total_results = data['search_information']['total_results']\n",
    "    except KeyError:\n",
    "        return 0\n",
    "    if total_results is None:\n",
    "        return 0\n",
    "    return total_results\n",
    "\n",
    "def get_num_cited_in_first_20_results(name):\n",
    "    client = GoogleScholarSearchResults({\"q\": '\"{}\"'.format(name), \"num\": 20})\n",
    "    data = client.get_json()\n",
    "    cited_by = 0\n",
    "    try:\n",
    "        organic_results = data['organic_results']\n",
    "    except KeyError:\n",
    "        return 0\n",
    "    for result in organic_results:\n",
    "        try:\n",
    "            cited_data = result['inline_links']['cited_by']\n",
    "            cited_by += cited_data.get('total') or 0\n",
    "        except KeyError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print('error adding scholar cited by')\n",
    "            print(e)\n",
    "            continue\n",
    "    return cited_by\n",
    "\n",
    "def get_num_google_search_results(name):\n",
    "    # test for nan\n",
    "    if name != name:\n",
    "        return name\n",
    "    client = GoogleSearchResults({\"q\": '\"{}\"'.format(name), \"num\": 20})\n",
    "    data = client.get_json()\n",
    "    try:\n",
    "        total_results = data['search_information']['total_results']\n",
    "    except KeyError:\n",
    "        return 0\n",
    "    if total_results is None:\n",
    "        return 0\n",
    "    return total_results\n",
    "\n",
    "# Serialize the results so that we can save them to disk in case \n",
    "# we need to redo analysis and avoid paying for more usage of the API\n",
    "def get_serialized_search_results(name):\n",
    "    if name != name:\n",
    "        return \"\"\n",
    "    retries = 0\n",
    "    while True:\n",
    "        try:\n",
    "            client = GoogleSearchResults({\"q\": '\"{}\"'.format(name), \"num\": 20})\n",
    "            search_data = client.get_json()\n",
    "            serialized = json.dumps(search_data)\n",
    "            return serialized\n",
    "        except Exception as e:\n",
    "            if retries == 0:\n",
    "                print('error getting serialized search results for {}, retrying'.format(name))\n",
    "                print(e)\n",
    "            if retries == 5:\n",
    "                raise\n",
    "            time.sleep(90)\n",
    "            retries += 1\n",
    "            \n",
    "def get_serialized_scholar_results(name):\n",
    "    retries = 0\n",
    "    while True:\n",
    "        try:\n",
    "            search_data = GoogleScholarSearchResults({\"q\": '\"{}\"'.format(name), \"num\": 20}).get_json()\n",
    "            serialized = json.dumps(search_data)\n",
    "            return serialized\n",
    "        except Exception as e:\n",
    "            if retries == 0:\n",
    "                print('error getting serialized scholar search results for {}, retrying'.format(name))\n",
    "                print(e)\n",
    "            if retries == 5:\n",
    "                raise\n",
    "            time.sleep(90)\n",
    "            retries += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make API calls to Phrasefinder to get absolute frequency and number of books for a given list of phrases\n",
    "PF_API_KEY = os.environ['PF_API_KEY']\n",
    "\n",
    "BATCH_ENDPOINT = \"https://api.phrasefinder.io/batch\"\n",
    "HEADERS = {\n",
    "    'X-API-Key': PF_API_KEY\n",
    "}\n",
    "MAX_PHRASES = 5\n",
    "GARBAGE_PHRASE = 'arstarstarstarst'\n",
    "\n",
    "\n",
    "def get_phrase_frequencies(names):\n",
    "    queries = [{'query': name if name == name else GARBAGE_PHRASE} for name in names]\n",
    "    top_level_params = {\n",
    "        'corpus': 'eng-us',\n",
    "        'topk': MAX_PHRASES,\n",
    "        'batch': queries\n",
    "    }\n",
    "    response = requests.post(BATCH_ENDPOINT, headers=HEADERS, json=top_level_params)\n",
    "    response_tsv = response.text\n",
    "    rows = response_tsv.split(\"\\n\")\n",
    "    rows = [row for row in rows if row]\n",
    "    if not rows:\n",
    "        return []\n",
    "    phrase_frequencies = []\n",
    "    i = 0\n",
    "    while i < len(rows):\n",
    "        line = rows[i]\n",
    "        status, num_lines = line.split()\n",
    "        if status != \"OK\":\n",
    "            print('error getting book freq')\n",
    "        num_lines = int(num_lines)\n",
    "        absolute_frequency_of_phrase = 0\n",
    "        num_books_containing_phrase = 0\n",
    "        start_of_phrase = i + 1\n",
    "        end_of_phrase = start_of_phrase + num_lines\n",
    "        for j in range(start_of_phrase, end_of_phrase):\n",
    "            chunk = rows[j]\n",
    "            phrase, match_score, volume_score, year_start, year_end, phrase_id, relative_frequency = chunk.split(\"\\t\")\n",
    "            absolute_frequency_of_phrase += int(match_score)\n",
    "            num_books_containing_phrase += int(volume_score)\n",
    "        phrase_frequency = {\n",
    "            'absolute_frequency': absolute_frequency_of_phrase,\n",
    "            'num_books': num_books_containing_phrase}\n",
    "        phrase_frequencies.append(phrase_frequency)\n",
    "        i += num_lines + 1\n",
    "    return phrase_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get phrasefinder results for families\n",
    "iteration = 0\n",
    "header = True\n",
    "for chunk in pd.read_csv(TWITTER_RESULTS_WITH_GOOGLE, chunksize=100):\n",
    "    print(\"On iteration \", iteration)\n",
    "    phrase_frequencies = get_phrase_frequencies(chunk.family)\n",
    "    chunk['absolute_frequency_of_family_in_books'] = [pf['absolute_frequency'] for pf in phrase_frequencies]\n",
    "    chunk['num_books_containing_family'] = [pf['num_books'] for pf in phrase_frequencies]\n",
    "    chunk.to_csv(TWITTER_RESULTS_WITH_BOOKS, header=header, mode=\"a\")\n",
    "    header = False\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get google search and scholar results for families\n",
    "iteration = 0\n",
    "header = True\n",
    "for chunk in pd.read_csv(TWITTER_RESULTS, chunksize=10):\n",
    "    print(\"On iteration \", iteration)\n",
    "    dump = chunk[['family']].copy()\n",
    "    dump['serialized_family_name_search_results'] = dump['family'].map(get_serialized_search_results)\n",
    "    dump['serialized_family_name_scholar_results'] = dump['family'].map(get_serialized_scholar_results)\n",
    "    dump.to_csv(FAMILY_GOOGLE_DUMP, header=header, mode=\"a\")\n",
    "    \n",
    "    chunk['num_family_name_google_scholar_results'] = chunk['family'].map(get_num_scholar_results)\n",
    "    chunk['num_family_name_cited_by_in_first_20_results'] = chunk['family'].map(get_num_cited_in_first_20_results)\n",
    "    chunk['num_family_name_google_search_results'] = chunk['family'].map(get_num_google_search_results)\n",
    "    chunk['num_family_name_google_search_results'] = chunk['family'].map(get_num_google_search_results)\n",
    "    chunk.to_csv(TWITTER_RESULTS_WITH_GOOGLE, header=header, mode=\"a\")\n",
    "    header = False\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check on output of serp for families\n",
    "df = pd.read_csv(TWITTER_RESULTS_WITH_GOOGLE)\n",
    "df\n",
    "dump = pd.read_csv(FAMILY_GOOGLE_DUMP)\n",
    "serialized = dump.iloc[0]\n",
    "result = json.loads(serialized.serialized_family_name_scholar_results)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clean up output of google scraping\n",
    "os.remove(FAMILY_GOOGLE_DUMP)\n",
    "os.remove(TWITTER_RESULTS_WITH_GOOGLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get phrasefinder results for IUCN insects\n",
    "iteration = 0\n",
    "header = True\n",
    "for chunk in pd.read_csv(OUTPUT_FILE, chunksize=100):\n",
    "    print(\"On iteration \", iteration)\n",
    "    scientific_name_phrase_frequencies = get_phrase_frequencies(chunk.scientificName)\n",
    "    common_name_phrase_frequencies = get_phrase_frequencies(chunk.commonName)\n",
    "    chunk['absolute_frequency_of_scientific_name_in_books'] = [pf['absolute_frequency'] for pf in scientific_name_phrase_frequencies]\n",
    "    chunk['num_books_containing_scientific_name'] = [pf['num_books'] for pf in scientific_name_phrase_frequencies]\n",
    "    chunk['absolute_frequency_of_common_name_in_books'] = [pf['absolute_frequency'] for pf in common_name_phrase_frequencies]\n",
    "    chunk['num_books_containing_common_name'] = [pf['num_books'] for pf in common_name_phrase_frequencies]\n",
    "    chunk.to_csv(IUCN_INSECTS_WITH_BOOKS, header=header, mode=\"a\")\n",
    "    header = False\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect phrasefinder results for IUCN insects\n",
    "df = pd.read_csv(IUCN_INSECTS_WITH_BOOKS)\n",
    "with_common_name = df[df.absolute_frequency_of_common_name_in_books > 0]\n",
    "with_scientific_name = df[df.absolute_frequency_of_scientific_name_in_books > 0]\n",
    "len(with_common_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform serialized Google search and scholar results into a format where we can get aggregate results\n",
    "iteration = 0\n",
    "header = False\n",
    "for chunk in pd.read_csv(INSECTS_WITH_COMMON_NAME_FILE, chunksize=10):\n",
    "    print(\"On iteration \", iteration)\n",
    "    dump = chunk[['scientificName', 'commonName']].copy()\n",
    "    dump['serialized_common_name_search_results'] = dump['commonName'].map(get_serialized_search_results)\n",
    "    dump['serialized_scientific_name_search_results'] = dump['scientificName'].map(get_serialized_search_results)\n",
    "    dump['serialized_scholar_results'] = dump['scientificName'].map(get_serialized_scholar_results)\n",
    "    dump.to_csv(SEARCH_RESULT_DUMP, header=header, mode=\"a\")\n",
    "    \n",
    "    chunk['num_google_scholar_results'] = chunk['scientificName'].map(get_num_scholar_results)\n",
    "    chunk['num_cited_by_in_first_20_results'] = chunk['scientificName'].map(get_num_cited_in_first_20_results)\n",
    "    chunk['num_common_name_google_search_results'] = chunk['commonName'].map(get_num_google_search_results)\n",
    "    chunk['num_scientific_name_google_search_results'] = chunk['scientificName'].map(get_num_google_search_results)\n",
    "    chunk.to_csv(OUTPUT_FILE, header=header, mode=\"a\")\n",
    "    header = False\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter down dupes from common_names file\n",
    "df = pd.read_csv(MAIN_NAMES_FILE)\n",
    "main = df[df.main]\n",
    "grouped = df.groupby(['scientificName', 'name']).size().reset_index()[['scientificName', 'name']]\n",
    "grouped.to_csv(INPUT_FILE)\n",
    "\n",
    "names = df.scientificName\n",
    "print(len(names), len(num_unique_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter down simple summary of all species to just insects\n",
    "df = pd.read_csv(RAW_FILE)\n",
    "insects = df[df.className.eq(\"INSECTA\")]\n",
    "insects.to_csv(INSECTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Join insects with common names and insert column to describe common name\n",
    "main_common_names = pd.read_csv(MAIN_NAMES_FILE)\n",
    "insects_with_common_name = pd.merge(insects, main_common_names, on='internalTaxonId', how='left', suffixes=('_insects_data', '_common_names_data'))\n",
    "# drop dupe columns from main_common_names\n",
    "insects_with_common_name = insects_with_common_name.loc[:, ~insects_with_common_name.columns.str.contains('_common_names_data')]\n",
    "insects_with_common_name = insects_with_common_name.loc[:, ~insects_with_common_name.columns.str.contains('^Unnamed')]\n",
    "insects_with_common_name.drop(['main'], axis=1, inplace=True)\n",
    "insects_with_common_name.rename({'scientificName_insects_data': 'scientificName', 'name': 'commonName'}, axis=1, inplace=True)\n",
    "insects_with_common_name.to_csv(\"insects_with_common_name.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter down to insects with common names\n",
    "df = pd.read_csv(INSECTS_WITH_COMMON_NAME_FILE)\n",
    "with_cn = df[~df.commonName.isnull()]\n",
    "print(len(df), len(with_cn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
